{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 183
    },
    "colab_type": "code",
    "id": "An8kHO-S5Z9b",
    "outputId": "639040d1-9fef-4308-92e9-9a668191f329"
   },
   "outputs": [],
   "source": [
    "#Using google drive to store training and testing data.\n",
    "#from google.colab import drive\n",
    "\n",
    "#Retrieving data as objects\n",
    "import pickle\n",
    "#Performing fast computations\n",
    "import numpy as np\n",
    "#Setting uyp Neural Networks\n",
    "import tensorflow as tf\n",
    "#Plotting graphs\n",
    "import matplotlib.pyplot as plt\n",
    "#Other utilities\n",
    "\n",
    "import cv2\n",
    "#Mounting drive that contains my data.\n",
    "#If you run this notebook, save the training data in a subdirectory called 'HW4-data'. Look at the next cell to better understand the structure of the directories.\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vaT-XSXu5wxd"
   },
   "source": [
    "## Training Set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "mKIJPnOs5b7-",
    "outputId": "61837704-1f50-41e5-9811-de2f8a97b3e2"
   },
   "outputs": [],
   "source": [
    "######### DONT BOTHER WITH THIS FOR NOW\n",
    "#training_set_generator = pickle.load(open( \"./drive/My Drive/DL Project/stickmap.p\", \"rb\" ))\n",
    "#training_set_discriminator1 = pickle.load(open(\"./drive/My Drive/DL Project/train1.p\", \"rb\"))\n",
    "#training_set_discriminator2 = pickle.load(open(\"./drive/My Drive/DL Project/train2.p\", \"rb\"))\n",
    "#training_set_discriminator3 = pickle.load(open(\"./drive/My Drive/DL Project/train3.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6MrPUxOcD5sC"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sd9bbPtMHqMj"
   },
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xp_jHjgM7v71"
   },
   "outputs": [],
   "source": [
    "#Add layer specific output\n",
    "def generator(z):\n",
    "    with tf.variable_scope(\"GAN/Generator\",reuse=tf.AUTO_REUSE):\n",
    "        #add noise to map. mostly not needed\n",
    "  \n",
    "        #Uncomment all print statements to check output shape \n",
    "        #print(z)\n",
    "    \n",
    "        ######## ENCODING #######\n",
    "  \n",
    "        #Convolution 0 and Max Pooling 0\n",
    "        gen_conv0 = tf.layers.conv2d(z, filters=64, kernel_size=3, strides=1, padding='SAME', activation=tf.nn.leaky_relu, kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(gen_conv0)\n",
    "        gen_pool0 = tf.layers.average_pooling2d(gen_conv0, pool_size=2, strides=2, padding=\"VALID\")\n",
    "        #print(gen_pool0)\n",
    "  \n",
    "        #Convolution 1 and Max Pooling 1\n",
    "        gen_conv1 = tf.layers.conv2d(gen_pool0, filters=128, kernel_size=3, strides=1, padding='SAME', activation=tf.nn.leaky_relu, kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(gen_conv1)\n",
    "        gen_pool1 = tf.layers.average_pooling2d(gen_conv1, pool_size=2, strides=2, padding=\"VALID\")\n",
    "        #print(gen_pool1)\n",
    "  \n",
    "        #Convolution 2 and Max Pooling 2\n",
    "        gen_conv2 = tf.layers.conv2d(gen_pool1, filters=256, kernel_size=3, strides=1, padding='SAME', activation=tf.nn.leaky_relu, kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(gen_conv2)\n",
    "        gen_pool2 = tf.layers.average_pooling2d(gen_conv2, pool_size=2, strides=2, padding=\"VALID\")\n",
    "        #print(gen_pool2)\n",
    "  \n",
    "        #Convolution 3 and Max Pooling 3\n",
    "        gen_conv3 = tf.layers.conv2d(gen_pool2, filters=512, kernel_size=3, strides=1, padding='SAME', activation=tf.nn.leaky_relu, kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(gen_conv3)\n",
    "        gen_pool3 = tf.layers.average_pooling2d(gen_conv3, pool_size=2, strides=2, padding=\"VALID\")\n",
    "        #print(gen_pool3)\n",
    "  \n",
    "        #Convolution 4 and Max Pooling 4\n",
    "        gen_conv4 = tf.layers.conv2d(gen_pool3, filters=512, kernel_size=3, strides=1, padding='SAME', activation=tf.nn.leaky_relu, kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(gen_conv4)\n",
    "        gen_pool4 = tf.layers.average_pooling2d(gen_conv4, pool_size=2, strides=2, padding=\"VALID\")\n",
    "        #print(gen_pool4)\n",
    "  \n",
    "        #Convolution 5 and Max Pooling 5\n",
    "        gen_conv5 = tf.layers.conv2d(gen_pool4, filters=512, kernel_size=3, strides=1, padding='SAME', activation=tf.nn.leaky_relu, kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(gen_conv5)\n",
    "        gen_pool5 = tf.layers.average_pooling2d(gen_conv5, pool_size=2, strides=2, padding=\"VALID\")\n",
    "        #print(gen_pool5)\n",
    "  \n",
    "        #Convolution 6 and Max Pooling 6\n",
    "        gen_conv6 = tf.layers.conv2d(gen_pool5, filters=512, kernel_size=3, strides=1, padding='SAME', activation=tf.nn.leaky_relu, kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(gen_conv6)\n",
    "        gen_pool6 = tf.layers.average_pooling2d(gen_conv6, pool_size=2, strides=2, padding=\"VALID\")\n",
    "        #print(gen_pool6)\n",
    "\n",
    "  \n",
    "        ######## DECODING #######\n",
    "  \n",
    "        #Deconvolution 0\n",
    "        gen_deconv0 = tf.layers.conv2d_transpose(gen_pool6, filters=512, kernel_size=(3,3), strides=(2,2), padding='SAME', kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(gen_deconv0)\n",
    "\n",
    "        #Deconvolution 1\n",
    "        gen_deconv1 = tf.layers.conv2d_transpose(gen_deconv0, filters=512, kernel_size=(3,3), strides=(2,2), padding='SAME', kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(gen_deconv1)\n",
    "  \n",
    "        #Deconvolution 2\n",
    "        gen_deconv2 = tf.layers.conv2d_transpose(gen_deconv1, filters=512, kernel_size=(3,3), strides=(2,2), padding='SAME', kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(gen_deconv2)\n",
    "  \n",
    "        #Deconvolution 3\n",
    "        gen_deconv3 = tf.layers.conv2d_transpose(gen_deconv2, filters=512, kernel_size=(3,3), strides=(2,2), padding='SAME', kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(gen_deconv3)\n",
    "  \n",
    "        #Deconvolution 4\n",
    "        gen_deconv4 = tf.layers.conv2d_transpose(gen_deconv3, filters=512, kernel_size=(3,3), strides=(2,2), padding='SAME', kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(gen_deconv4)\n",
    "  \n",
    "        #Deconvolution 5\n",
    "        gen_deconv5 = tf.layers.conv2d_transpose(gen_deconv4, filters=512, kernel_size=(3,3), strides=(2,2), padding='SAME', kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(gen_deconv5)\n",
    "  \n",
    "        #Deconvolution 6\n",
    "        gen_deconv6 = tf.layers.conv2d_transpose(gen_deconv5, filters=512, kernel_size=(3,3), strides=(2,2), padding='SAME', kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(gen_deconv6)\n",
    "  \n",
    "        #Deconvolution 7\n",
    "        gen_deconv7 = tf.layers.conv2d_transpose(gen_deconv6, filters=3, kernel_size=(3,3), strides=(1,1), padding='SAME', kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(gen_deconv7)\n",
    "  \n",
    "    \n",
    "        return gen_deconv7  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ggf0QoDqfnrM"
   },
   "source": [
    "### Discriminator 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3UauTa-PfnVE"
   },
   "outputs": [],
   "source": [
    "#Add layer specific output\n",
    "\n",
    "def discriminator_1(z):\n",
    "    with tf.variable_scope(\"GAN/Discriminator1\",reuse=tf.AUTO_REUSE):\n",
    "  \n",
    "        #print(z)\n",
    "        #Uncomment all print statements to check output shapes\n",
    "    \n",
    "    \n",
    "        #Convolution 0 and Max Pooling 0\n",
    "        dis1_conv0 = tf.layers.conv2d(z, filters=64, kernel_size=3, strides=1, padding='SAME', activation=tf.nn.relu, kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(dis1_conv0)\n",
    "        dis1_pool0 = tf.layers.average_pooling2d(dis1_conv0, pool_size=2, strides=2, padding=\"VALID\")\n",
    "        #print(dis1_pool0)\n",
    "  \n",
    "        #Convolution 1 and Max Pooling 1\n",
    "        dis1_conv1 = tf.layers.conv2d(dis1_pool0, filters=128, kernel_size=3, strides=1, padding='SAME', activation=tf.nn.relu, kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(dis1_conv1)\n",
    "        dis1_pool1 = tf.layers.average_pooling2d(dis1_conv1, pool_size=2, strides=2, padding=\"VALID\")\n",
    "        #print(dis1_pool1)\n",
    "  \n",
    "        #Convolution 2 and Max Pooling 2\n",
    "        dis1_conv2 = tf.layers.conv2d(dis1_pool1, filters=256, kernel_size=3, strides=1, padding='SAME', activation=tf.nn.relu, kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(dis1_conv2)\n",
    "        dis1_pool2 = tf.layers.average_pooling2d(dis1_conv2, pool_size=2, strides=2, padding=\"VALID\")\n",
    "        #print(dis1_pool2)\n",
    "  \n",
    "        #Convolution 3 and Max Pooling 3\n",
    "        dis1_conv3 = tf.layers.conv2d(dis1_pool2, filters=512, kernel_size=3, strides=1, padding='SAME', activation=tf.nn.relu, kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(dis1_conv3)\n",
    "        dis1_pool3 = tf.layers.average_pooling2d(dis1_conv3, pool_size=2, strides=2, padding=\"VALID\")\n",
    "        #print(dis1_pool3)\n",
    "  \n",
    "        #Convolution 4 and Max Pooling 4\n",
    "        dis1_conv4 = tf.layers.conv2d(dis1_pool3, filters=512, kernel_size=3, strides=1, padding='SAME', activation=tf.nn.relu, kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(dis1_conv4)\n",
    "        dis1_pool4 = tf.layers.average_pooling2d(dis1_conv4, pool_size=2, strides=2, padding=\"VALID\")\n",
    "        #print(dis1_pool4)\n",
    "  \n",
    "        #Convolution 5 and Max Pooling 5\n",
    "        dis1_conv5 = tf.layers.conv2d(dis1_pool4, filters=512, kernel_size=3, strides=1, padding='SAME', activation=tf.nn.relu, kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(dis1_conv5)\n",
    "        dis1_pool5 = tf.layers.average_pooling2d(dis1_conv5, pool_size=2, strides=2, padding=\"VALID\")\n",
    "        #print(dis1_pool5)\n",
    "  \n",
    "        #Convolution 6 and Max Pooling 6\n",
    "        dis1_conv6 = tf.layers.conv2d(dis1_pool5, filters=512, kernel_size=3, strides=1, padding='SAME', activation=tf.nn.relu, kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(dis1_conv6)\n",
    "        dis1_pool6 = tf.layers.average_pooling2d(dis1_conv6, pool_size=2, strides=2, padding=\"VALID\")\n",
    "        #print(dis1_pool6)\n",
    "  \n",
    "        #Final Convolution and Flattening\n",
    "        dis1_conv7 = tf.layers.conv2d(dis1_pool6, filters=1, kernel_size=3, strides=1, padding='SAME', activation=tf.nn.relu, kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(dis1_conv7)\n",
    "        dis1_flat = tf.layers.flatten(dis1_conv7)\n",
    "        #print(dis1_flat)\n",
    "        dis1_logit = tf.layers.dense(dis1_flat, 1, kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(dis1_logit)\n",
    "    \n",
    "        return dis1_logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WO8lQTbyjHCT"
   },
   "source": [
    "### Discriminator 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iNGtnCUyjGxP"
   },
   "outputs": [],
   "source": [
    "#Add layer specific output\n",
    "\n",
    "def discriminator_2(z):\n",
    "    with tf.variable_scope(\"GAN/Discriminator2\",reuse=tf.AUTO_REUSE):\n",
    "    \n",
    "        #print(z)\n",
    "    \n",
    "        #Uncomment all print statements to check output shapes\n",
    "  \n",
    "        #Convolution 0 and Max Pooling 0\n",
    "        dis2_conv0 = tf.layers.conv2d(z, filters=64, kernel_size=3, strides=1, padding='SAME', activation=tf.nn.relu, kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(dis2_conv0)\n",
    "        dis2_pool0 = tf.layers.average_pooling2d(dis2_conv0, pool_size=2, strides=2, padding=\"VALID\")\n",
    "        #print(dis2_pool0)\n",
    "  \n",
    "        #Convolution 1 and Max Pooling 1\n",
    "        dis2_conv1 = tf.layers.conv2d(dis2_pool0, filters=128, kernel_size=3, strides=1, padding='SAME', activation=tf.nn.relu, kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(dis2_conv1)\n",
    "        dis2_pool1 = tf.layers.average_pooling2d(dis2_conv1, pool_size=2, strides=2, padding=\"VALID\")\n",
    "        #print(dis2_pool1)\n",
    "  \n",
    "        #Convolution 2 and Max Pooling 2\n",
    "        dis2_conv2 = tf.layers.conv2d(dis2_pool1, filters=256, kernel_size=3, strides=1, padding='SAME', activation=tf.nn.relu, kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(dis2_conv2)\n",
    "        dis2_pool2 = tf.layers.average_pooling2d(dis2_conv2, pool_size=2, strides=2, padding=\"VALID\")\n",
    "        #print(dis2_pool2)\n",
    "  \n",
    "        #Convolution 3 and Max Pooling 3\n",
    "        dis2_conv3 = tf.layers.conv2d(dis2_pool2, filters=512, kernel_size=3, strides=1, padding='SAME', activation=tf.nn.relu, kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(dis2_conv3)\n",
    "        dis2_pool3 = tf.layers.average_pooling2d(dis2_conv3, pool_size=2, strides=2, padding=\"VALID\")\n",
    "        #print(dis2_pool3)\n",
    "  \n",
    "        #Convolution 4 and Max Pooling 4\n",
    "        dis2_conv4 = tf.layers.conv2d(dis2_pool3, filters=512, kernel_size=3, strides=1, padding='SAME', activation=tf.nn.relu, kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(dis2_conv4)\n",
    "        dis2_pool4 = tf.layers.average_pooling2d(dis2_conv4, pool_size=2, strides=2, padding=\"VALID\")\n",
    "        #print(dis2_pool4)\n",
    "  \n",
    "        #Convolution 5 and Max Pooling 5\n",
    "        dis2_conv5 = tf.layers.conv2d(dis2_pool4, filters=512, kernel_size=3, strides=1, padding='SAME', activation=tf.nn.relu, kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(dis2_conv5)\n",
    "        dis2_pool5 = tf.layers.average_pooling2d(dis2_conv5, pool_size=2, strides=2, padding=\"VALID\")\n",
    "        #print(dis2_pool5)\n",
    "  \n",
    "        #Convolution 6 and Max Pooling 6\n",
    "        dis2_conv6 = tf.layers.conv2d(dis2_pool5, filters=512, kernel_size=3, strides=1, padding='SAME', activation=tf.nn.relu, kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(dis2_conv6)\n",
    "        dis2_pool6 = tf.layers.average_pooling2d(dis2_conv6, pool_size=2, strides=2, padding=\"VALID\")\n",
    "        #print(dis2_pool6)\n",
    "  \n",
    "        #Final Convolution and Flattening\n",
    "        dis2_conv7 = tf.layers.conv2d(dis2_pool6, filters=1, kernel_size=3, strides=1, padding='SAME', activation=tf.nn.relu, kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(dis2_conv7)\n",
    "        dis2_flat = tf.layers.flatten(dis2_conv7)\n",
    "        #print(dis2_flat)\n",
    "        dis2_logit = tf.layers.dense(dis2_flat, 1, kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(dis2_logit)\n",
    "  \n",
    "        return dis2_logit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Bed / Running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "colab_type": "code",
    "id": "x5omAzdnPo8l",
    "outputId": "54011987-4466-4c43-fe03-47f5f69a9b46"
   },
   "outputs": [],
   "source": [
    "#Graph setup\n",
    "tf.keras.backend.set_image_data_format('channels_last')\n",
    "tf.reset_default_graph()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "colab_type": "code",
    "id": "x5omAzdnPo8l",
    "outputId": "54011987-4466-4c43-fe03-47f5f69a9b46"
   },
   "outputs": [],
   "source": [
    "#Placeholders\n",
    "d1_data = tf.placeholder(tf.float32, shape=[None,640,640,6]) #img and map\n",
    "d2_data = tf.placeholder(tf.float32, shape=[None,640,640,6])\n",
    "posemap = tf.placeholder(tf.float32, shape=[None,640,640,3])\n",
    "noise = tf.placeholder(tf.float32, shape=[None,640,640,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "colab_type": "code",
    "id": "x5omAzdnPo8l",
    "outputId": "54011987-4466-4c43-fe03-47f5f69a9b46"
   },
   "outputs": [],
   "source": [
    "#Pathways\n",
    "discriminator_1_image_decision = discriminator_1(d1_data)\n",
    "discriminator_2_image_decision = discriminator_2(d2_data)\n",
    "generated_image=generator(tf.concat([noise,posemap], 3))\n",
    "discriminator_1_generated_decision = discriminator_1(tf.concat([generated_image,posemap], 3))\n",
    "discriminator_2_generated_decision = discriminator_2(tf.concat([generated_image,posemap], 3))\n",
    "      #Check shapes\n",
    "print(discriminator_1_generated_decision)\n",
    "print(discriminator_2_generated_decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "colab_type": "code",
    "id": "x5omAzdnPo8l",
    "outputId": "54011987-4466-4c43-fe03-47f5f69a9b46"
   },
   "outputs": [],
   "source": [
    "#Collecting variables\n",
    "generator_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope=\"GAN/Generator\")\n",
    "discriminator_1_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope=\"GAN/Discriminator1\")\n",
    "discriminator_2_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope=\"GAN/Discriminator2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "colab_type": "code",
    "id": "x5omAzdnPo8l",
    "outputId": "54011987-4466-4c43-fe03-47f5f69a9b46"
   },
   "outputs": [],
   "source": [
    "#LOSSES\n",
    "\n",
    "# Normal\n",
    "\n",
    "\n",
    "#discriminator_1_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=discriminator_1_image_decision,labels=tf.ones_like(discriminator_1_image_decision)) + \n",
    "#                                      tf.nn.sigmoid_cross_entropy_with_logits(logits=discriminator_1_generated_decision, labels=tf.zeros_like(discriminator_1_generated_decision)))\n",
    "#discriminator_2_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=discriminator_2_image_decision,labels=tf.ones_like(discriminator_2_image_decision)) + \n",
    "#                                      tf.nn.sigmoid_cross_entropy_with_logits(logits=discriminator_2_generated_decision, labels=tf.zeros_like(discriminator_2_generated_decision)))\n",
    "#generator_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=discriminator_1_generated_decision,labels=tf.ones_like(discriminator_1_generated_decision)) + \n",
    "#                                  tf.nn.sigmoid_cross_entropy_with_logits(logits=discriminator_2_generated_decision,labels=tf.ones_like(discriminator_2_generated_decision)))\n",
    "\n",
    "\n",
    "#Losses - Wasserstien (add feature matching loss)\n",
    "discriminator_1_loss = tf.reduce_mean(discriminator_1_image_decision) - tf.reduce_mean(discriminator_1_generated_decision)\n",
    "discriminator_2_loss = tf.reduce_mean(discriminator_2_image_decision) - tf.reduce_mean(discriminator_2_generated_decision)\n",
    "generator_loss =  -tf.reduce_mean(discriminator_1_generated_decision) - tf.reduce_mean(discriminator_2_generated_decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "colab_type": "code",
    "id": "x5omAzdnPo8l",
    "outputId": "54011987-4466-4c43-fe03-47f5f69a9b46"
   },
   "outputs": [],
   "source": [
    "#Optimizer\n",
    "discriminator_1_optimizer = (tf.train.RMSPropOptimizer(learning_rate=1e-4).minimize(-discriminator_1_loss, var_list=discriminator_1_variables))\n",
    "discriminator_2_optimizer = (tf.train.RMSPropOptimizer(learning_rate=1e-4).minimize(-discriminator_2_loss, var_list=discriminator_2_variables))\n",
    "generator_optimizer = (tf.train.RMSPropOptimizer(learning_rate=1e-4).minimize(-generator_loss, var_list=generator_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "colab_type": "code",
    "id": "x5omAzdnPo8l",
    "outputId": "54011987-4466-4c43-fe03-47f5f69a9b46"
   },
   "outputs": [],
   "source": [
    "#Gradient clipping\n",
    "clip_discriminator_1_gradient = [p.assign(tf.clip_by_value(p, -0.01, 0.01)) for p in discriminator_1_variables]\n",
    "clip_discriminator_2_gradient = [p.assign(tf.clip_by_value(p, -0.01, 0.01)) for p in discriminator_2_variables]\n",
    "\n",
    "\n",
    "#Add FM loss to Generator- wont take time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "colab_type": "code",
    "id": "x5omAzdnPo8l",
    "outputId": "54011987-4466-4c43-fe03-47f5f69a9b46"
   },
   "outputs": [],
   "source": [
    "#DUMMY DATA - change these data points - read file during epochs\n",
    "a = np.random.rand(10,640,640,6)\n",
    "b = np.random.rand(10,640,640,3)\n",
    "c = np.random.rand(10,640,640,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "colab_type": "code",
    "id": "x5omAzdnPo8l",
    "outputId": "54011987-4466-4c43-fe03-47f5f69a9b46"
   },
   "outputs": [],
   "source": [
    "#Solving - put this in loop - write code to save model - ask bhushan. \n",
    "sess.run(tf.initialize_all_variables())\n",
    "for i in range(10):\n",
    "    print(i)\n",
    "  \n",
    "\n",
    "  \n",
    "    _, d1_loss, _ = sess.run([discriminator_1_optimizer, discriminator_1_loss, clip_discriminator_1_gradient], feed_dict={d1_data:a, d2_data:a, posemap:b, noise:c})\n",
    "    print(d1_loss)\n",
    "    _, d2_loss, _ = sess.run([discriminator_2_optimizer, discriminator_2_loss, clip_discriminator_2_gradient], feed_dict={d1_data:a, d2_data:a, posemap:b, noise:c})\n",
    "    print(d2_loss)\n",
    "    _, gen_loss = sess.run([generator_optimizer, generator_loss],feed_dict={d1_data:a, d2_data:a, posemap:b, noise:c} )\n",
    "    print(gen_loss)\n",
    "\n",
    "  \n",
    "  \n",
    "#Printing section\n",
    "#print('=============================================')\n",
    "#print(discriminator_1_image_decision)\n",
    "#print(discriminator_2_image_decision)\n",
    "#print(generated_image)\n",
    "#print(discriminator_1_generated_decision)\n",
    "#print(discriminator_2_generated_decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "colab_type": "code",
    "id": "x5omAzdnPo8l",
    "outputId": "54011987-4466-4c43-fe03-47f5f69a9b46"
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TNI7uFx3QjBT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "name": "DL Project.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
